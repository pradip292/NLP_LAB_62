{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing NLTK library\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#string\n",
    "import random\n",
    "\n",
    "mystring= \"\"\"\n",
    "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard. How was your day? I hope you have a good day today. Does it inspire you? I am inspired. I am inspired by you. You are my inspiration. I am inspired by your work. I am inspired by your work ethics. Now I am going to sleep. Goodnight. Have a great day. I will see you tomorrow. I will see you later. I will see you soon. I will see you in the evening. I will see you in the morning. I will see you in the afternoon.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pradip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nHello Mr. Smith, how are you doing today?',\n",
       " 'The weather is great, and city is awesome.',\n",
       " 'The sky is pinkish-blue.',\n",
       " \"You shouldn't eat cardboard.\",\n",
       " 'How was your day?',\n",
       " 'I hope you have a good day today.',\n",
       " 'Does it inspire you?',\n",
       " 'I am inspired.',\n",
       " 'I am inspired by you.',\n",
       " 'You are my inspiration.',\n",
       " 'I am inspired by your work.',\n",
       " 'I am inspired by your work ethics.',\n",
       " 'Now I am going to sleep.',\n",
       " 'Goodnight.',\n",
       " 'Have a great day.',\n",
       " 'I will see you tomorrow.',\n",
       " 'I will see you later.',\n",
       " 'I will see you soon.',\n",
       " 'I will see you in the evening.',\n",
       " 'I will see you in the morning.',\n",
       " 'I will see you in the afternoon.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#senteces\n",
    "sent_tokenize(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'city',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '.',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard',\n",
       " '.',\n",
       " 'How',\n",
       " 'was',\n",
       " 'your',\n",
       " 'day',\n",
       " '?',\n",
       " 'I',\n",
       " 'hope',\n",
       " 'you',\n",
       " 'have',\n",
       " 'a',\n",
       " 'good',\n",
       " 'day',\n",
       " 'today',\n",
       " '.',\n",
       " 'Does',\n",
       " 'it',\n",
       " 'inspire',\n",
       " 'you',\n",
       " '?',\n",
       " 'I',\n",
       " 'am',\n",
       " 'inspired',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'inspired',\n",
       " 'by',\n",
       " 'you',\n",
       " '.',\n",
       " 'You',\n",
       " 'are',\n",
       " 'my',\n",
       " 'inspiration',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'inspired',\n",
       " 'by',\n",
       " 'your',\n",
       " 'work',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'inspired',\n",
       " 'by',\n",
       " 'your',\n",
       " 'work',\n",
       " 'ethics',\n",
       " '.',\n",
       " 'Now',\n",
       " 'I',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'sleep',\n",
       " '.',\n",
       " 'Goodnight',\n",
       " '.',\n",
       " 'Have',\n",
       " 'a',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'see',\n",
       " 'you',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'see',\n",
       " 'you',\n",
       " 'later',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'see',\n",
       " 'you',\n",
       " 'soon',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'see',\n",
       " 'you',\n",
       " 'in',\n",
       " 'the',\n",
       " 'evening',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'see',\n",
       " 'you',\n",
       " 'in',\n",
       " 'the',\n",
       " 'morning',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'see',\n",
       " 'you',\n",
       " 'in',\n",
       " 'the',\n",
       " 'afternoon',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenize words\n",
    "tokenizestring = word_tokenize(mystring)\n",
    "tokenizestring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Pradip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering the stop words\n",
    "filtered_list = []\n",
    "for word in tokenizestring:\n",
    "   if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [\n",
    "    word for word in tokenizestring if word.casefold() not in stop_words\n",
    "]\n",
    "filtered_list\n",
    "\n",
    "filteredstring= ' '.join(filtered_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_words = [stemmer.stem(word) for word in filtered_list]\n",
    "stemmed_words\n",
    "\n",
    "stemmedstring= ' '.join(stemmed_words)\n",
    "stemmedstring\n",
    "\n",
    "words_in_quote = word_tokenize(stemmedstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pradip\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('hello', 'NN'),\n",
       " ('mr.', 'NN'),\n",
       " ('smith', 'NN'),\n",
       " (',', ','),\n",
       " ('today', 'NN'),\n",
       " ('?', '.'),\n",
       " ('weather', 'RB'),\n",
       " ('great', 'JJ'),\n",
       " (',', ','),\n",
       " ('citi', 'JJ'),\n",
       " ('awesom', 'NN'),\n",
       " ('.', '.'),\n",
       " ('sky', 'JJ'),\n",
       " ('pinkish-blu', 'NN'),\n",
       " ('.', '.'),\n",
       " (\"n't\", 'RB'),\n",
       " ('eat', 'VB'),\n",
       " ('cardboard', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('day', 'NN'),\n",
       " ('?', '.'),\n",
       " ('hope', 'NN'),\n",
       " ('good', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('today', 'NN'),\n",
       " ('.', '.'),\n",
       " ('inspir', 'VB'),\n",
       " ('?', '.'),\n",
       " ('inspir', 'NN'),\n",
       " ('.', '.'),\n",
       " ('inspir', 'NN'),\n",
       " ('.', '.'),\n",
       " ('inspir', 'NN'),\n",
       " ('.', '.'),\n",
       " ('inspir', 'JJ'),\n",
       " ('work', 'NN'),\n",
       " ('.', '.'),\n",
       " ('inspir', 'JJ'),\n",
       " ('work', 'NN'),\n",
       " ('ethic', 'NN'),\n",
       " ('.', '.'),\n",
       " ('go', 'VB'),\n",
       " ('sleep', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('goodnight', 'NN'),\n",
       " ('.', '.'),\n",
       " ('great', 'JJ'),\n",
       " ('day', 'NN'),\n",
       " ('.', '.'),\n",
       " ('see', 'VB'),\n",
       " ('tomorrow', 'NN'),\n",
       " ('.', '.'),\n",
       " ('see', 'VB'),\n",
       " ('later', 'RB'),\n",
       " ('.', '.'),\n",
       " ('see', 'VB'),\n",
       " ('soon', 'RB'),\n",
       " ('.', '.'),\n",
       " ('see', 'VB'),\n",
       " ('even', 'RB'),\n",
       " ('.', '.'),\n",
       " ('see', 'VB'),\n",
       " ('morn', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('see', 'VB'),\n",
       " ('afternoon', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part of speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(words_in_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pradip\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'mr.',\n",
       " 'smith',\n",
       " ',',\n",
       " 'today',\n",
       " '?',\n",
       " 'weather',\n",
       " 'great',\n",
       " ',',\n",
       " 'citi',\n",
       " 'awesom',\n",
       " '.',\n",
       " 'sky',\n",
       " 'pinkish-blu',\n",
       " '.',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard',\n",
       " '.',\n",
       " 'day',\n",
       " '?',\n",
       " 'hope',\n",
       " 'good',\n",
       " 'day',\n",
       " 'today',\n",
       " '.',\n",
       " 'inspir',\n",
       " '?',\n",
       " 'inspir',\n",
       " '.',\n",
       " 'inspir',\n",
       " '.',\n",
       " 'inspir',\n",
       " '.',\n",
       " 'inspir',\n",
       " 'work',\n",
       " '.',\n",
       " 'inspir',\n",
       " 'work',\n",
       " 'ethic',\n",
       " '.',\n",
       " 'go',\n",
       " 'sleep',\n",
       " '.',\n",
       " 'goodnight',\n",
       " '.',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'see',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'see',\n",
       " 'later',\n",
       " '.',\n",
       " 'see',\n",
       " 'soon',\n",
       " '.',\n",
       " 'see',\n",
       " 'even',\n",
       " '.',\n",
       " 'see',\n",
       " 'morn',\n",
       " '.',\n",
       " 'see',\n",
       " 'afternoon',\n",
       " '.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words_in_quote]\n",
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'mr',\n",
       " '.',\n",
       " 'smith',\n",
       " ',',\n",
       " 'today',\n",
       " '?',\n",
       " 'weather',\n",
       " 'great',\n",
       " ',',\n",
       " 'citi',\n",
       " 'awesom',\n",
       " '.',\n",
       " 'sky',\n",
       " 'pinkish',\n",
       " '-',\n",
       " 'blu',\n",
       " '.',\n",
       " 'not',\n",
       " 'eat',\n",
       " 'cardboard',\n",
       " '.',\n",
       " 'day',\n",
       " '?',\n",
       " 'hope',\n",
       " 'good',\n",
       " 'day',\n",
       " 'today',\n",
       " '.',\n",
       " 'inspir',\n",
       " '?',\n",
       " 'inspir',\n",
       " '.',\n",
       " 'inspir',\n",
       " '.',\n",
       " 'inspir',\n",
       " '.',\n",
       " 'inspir',\n",
       " 'work',\n",
       " '.',\n",
       " 'inspir',\n",
       " 'work',\n",
       " 'ethic',\n",
       " '.',\n",
       " 'go',\n",
       " 'sleep',\n",
       " '.',\n",
       " 'goodnight',\n",
       " '.',\n",
       " 'great',\n",
       " 'day',\n",
       " '.',\n",
       " 'see',\n",
       " 'tomorrow',\n",
       " '.',\n",
       " 'see',\n",
       " 'later',\n",
       " '.',\n",
       " 'see',\n",
       " 'soon',\n",
       " '.',\n",
       " 'see',\n",
       " 'even',\n",
       " '.',\n",
       " 'see',\n",
       " 'morn',\n",
       " '.',\n",
       " 'see',\n",
       " 'afternoon',\n",
       " '.']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#lemmatization using spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(stemmedstring)\n",
    "lemmatized_words = [token.lemma_ for token in doc]\n",
    "lemmatized_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
